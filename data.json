{"topics": ["dropout recovery topic condition medlda anchor robust regression cij pairs lasso measurements norm momentum nag returns support sparsity recurrent signal", "reward policy topic words sgd infvoc psr mixing vertices assignment proportional word pca temporal lda rates modes online variational soft", "backup group policy subspace kernel coding node dictionary sparse feature invariance mri label kernels convex pooling scheds topical lasso propagation", "dropout pmix dropconnect maxout coco rule transition boolean units autoencoders manifold autoencoder var tuning activation series layer sulda channel ulda", "topic search policy bekk ddp rbm dae songs meme annotation denoising novel dirichlet cpsr variational guiding bmdc gist patterns compressed", "kernels kernel customer abc reinforcement pgbm environment hamming neighbor graph triggering internal feature embedding subtle ibp policy cursor road meibp"], "papers": [{"pdf_url": "http://jmlr.org/proceedings/papers/v28/kamyshanska13.pdf", "title": "On autoencoder scoring", "topics": [0.015839622087954353, 0.015817113974139291, 0.015918656790619285, 0.92069303837375127, 0.015926523794541191, 0.015805044978994584], "most_common_string": "autoencoder data scores autoencoders energy learning training using activation hidden model function models class may log bengio sigmoid linear vector reconstruction factored assign one error example dynamical vincent conference memisevic scoring show score binary alain const activations international hinton train machine systems compute case rbm used functions weights learn representations gradient classes also performance probabilistic unnormalized units neural input denoising unit information shown features number based processing deep parameters trained viewed aes rifai variety allows use work criterion swersky energies squared larochelle exp typically section networks contractive like way outputs figure good term weight since shows normalizing icml potential multiple", "authors": "Hanna Kamyshanska; Roland Memisevic", "thumbnail_path": "thumbnails/On autoencoder scoring.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/pascanu13.pdf", "title": "On the difficulty of training Recurrent Neural Networks", "topics": [0.92893961506849898, 0.014189300068138733, 0.014231216209820647, 0.014206350654434223, 0.014249869631543016, 0.014183648367564534], "most_common_string": "model gradients recurrent term one gradient state neural problem networks exploding wrec norm vanishing time training long error clipping use learning see order network bengio step input regularization length problems attractor unit would mikolov temporal large rate jaeger change two dynamical systems hidden sequence boundary sequences direction descent task success doya results win explode also art hypothesis basins used sutskever matrix msgd behaviour train attraction crossing given jacobian bifurcation tanh value components proposed rnn using though test pathological asymptotic therefore curvature figure information maps derivative practice language note deal schmidhuber single models case matrices diag address values learn means short", "authors": "Razvan Pascanu; Tomas Mikolov; Yoshua Bengio", "thumbnail_path": "thumbnails/On the difficulty of training Recurrent Neural Networks.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/goodfellow13.pdf", "title": "Maxout Networks", "topics": [0.022105683924151662, 0.022082196743977323, 0.022316917541975637, 0.88933242235847088, 0.022121444920450293, 0.02204133451097421], "most_common_string": "maxout dropout training model set error units networks test averaging best mnist function pooling network learning validation deep gradient linear state models activation two hidden layer layers neural number optimization large approximation trained hinton may performance many using methods approximate convolutional rate continuous well use dataset weights applied unit train figure max art table conference bagging obtained parameters likelihood stochastic international softmax inputs given also arbitrarily pooled fergus sgd zeiler krizhevsky new method value bengio used obtain convex mask results small prediction connected positive times feature data rates proceedings geometric piecewise digits srivastava mlp make architectures input provided single consists", "authors": "Ian Goodfellow; David Warde-Farley; Mehdi Mirza; Aaron Courville; Yoshua Bengio", "thumbnail_path": "thumbnails/Maxout Networks.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/bardenet13.pdf", "title": "Collaborative hyperparameter tuning", "topics": [0.016464292054697759, 0.016430905099755293, 0.01670888228066426, 0.91719476084783114, 0.016667631996110936, 0.016533527720940664], "most_common_string": "tuning hyperparameter optimization hyperparameters algorithm number collaborative learning error function problems section figure surrogate problem scot methods two model ranking log average set similar machine datasets search rankings validation results experiment point default space also smbo terms used dataset global setup experiments quality random new conference feature training rank given target note common method criterion gaussian features points adaboost best described strategy value using posterior separate applied choice sequential international algorithms better since one errors means kernel bayesian thus user grid step bergstra presented past prior descriptors propose optimal research strategies proceedings principal knowledge automatic see available three journal use", "authors": "Rmi Bardenet; Mtys Brendel; Balazs Kegl; Michele Sebag", "thumbnail_path": "thumbnails/Collaborative hyperparameter tuning.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/memisevic13.pdf", "title": "Learning mid-level representations of objects by harnessing the aperture problem", "topics": [0.015046509095092732, 0.015002847725029536, 0.92467545855713795, 0.015145772067174398, 0.015102919744375146, 0.015026492811190192], "most_common_string": "features learning images invariant aperture training videos model learn data complex using transformations motion set memisevic cell figure subspace models pooling fourier components problem image approach objects example transformation object video work show use two one also input frames representation shows energy multiple layer neural test computation subspaces trained shown across invariance form harnessing feature natural rotations makes representations used related autoencoder recognition encode amounts matrix translations bilinear since may hoyer olshausen class random vincent vision frame see fleet code norb allows cadieu hinton factored still rotation linear top extracting rao note phase typically based responses noisy independent well possible", "authors": "Roland Memisevic; Georgios Exarchakis", "thumbnail_path": "thumbnails/Learning mid-level representations of objects by harnessing the aperture problem.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/krause13.pdf", "title": "Approximation properties of DBNs with binary hidden units and real-valued visible units", "topics": [0.023217095255695659, 0.023136142311923538, 0.023326934684837212, 0.88371457180419621, 0.023382670083447962, 0.023222585859899458], "most_common_string": "pmix distributions binary visible mixture units distribution hidden approximation dbns family properties log variables qmix mixtures bound exp gaussian let mix arbitrarily thus model theorem rbms hinton given exponential conditional results neural every compact dbn exists deep restricted belief probability roux networks lemma rbm follows layers show equation one variance learning well positive following furthermore bengio constant computation case copenhagen vectors two get holds energy number parameters joint corresponding layer normalization components strictly dvj models upper subset approximated continuous boltzmann densities written however salakhutdinov independent used density representational power step universal second machines information element practice shown formula kldivergence proceedings", "authors": "Oswin Krause; Asja Fischer; Tobias Glasmachers; Christian Igel", "thumbnail_path": "thumbnails/Approximation properties of DBNs with binary hidden units and real-valued visible units.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/bengio13.pdf", "title": "Better Mixing via Deep Representations", "topics": [0.01390174283533328, 0.93014409616778138, 0.014000497174065525, 0.013994109534002503, 0.014014390896848188, 0.01394516339196928], "most_common_string": "samples learning better deep mixing representations one modes would deeper algorithms levels input bengio distribution factors sampling higher volume space data representation layer interpolating manifold markov disentangling features classes also hypothesis hinton good used model mnist using manifolds via cae raw dbn rifai pages layers hidden mode another proceedings tfd experiments underlying examples associated chain likely example machine lecun quality top obtained points tend models depth results rbm may hypotheses mcmc idea training object various many density conference near report noise generated international technical two correspond interpolation could unlikely boltzmann vincent second lower mix image nearest whereas variation faster neighbors", "authors": "Yoshua Bengio; Gregoire Mesnil; Yann Dauphin; Salah Rifai", "thumbnail_path": "thumbnails/Better Mixing via Deep Representations.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13a.pdf", "title": "Fast dropout training", "topics": [0.88324021058908375, 0.023188543120413629, 0.023508564401240058, 0.023577767069079623, 0.023247946938898475, 0.023236967881284331], "most_common_string": "dropout training fast neural gaussian time approximation using objective real log data hidden loss function unit random variance sampling input noise test table gradient features networks regression samples validation error proceedings used trained let also logistic manning see learning respect datasets variable results errors hinton train wang figure feature still shown network normal methods computed approach plain units without output approximate applied exact method instead directly inputs linear one performance show expectation distribution taking case mnist less dimensions much faster small maxout sgd integrating regularization may number softmax mean idea sentiment since performs times rate empirically set label make several", "authors": "Sida Wang; Christopher Manning", "thumbnail_path": "thumbnails/Fast dropout training.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/kolar13.pdf", "title": "Feature Selection in High-Dimensional Classification", "topics": [0.016361986203238699, 0.016048628658308351, 0.016178742741989879, 0.016078392606953661, 0.016152287790659355, 0.91917996199884999], "most_common_string": "problem log sample set theorem size sign selection road discriminant optimization analysis solution conditions sparsity variable matrix lemma linear distance hamming let min constant results vector scaled section feature result procedure regime estimator scaling recover denote given equal consistency version following proof high sharp parameter support subplot corresponds independent three fan sublinear paper show estimation dimensional characterize power fractional second theoretical least observe simulation oracle risk condition behavior two particular figure information max provide correlation kkt denotes probability class covariance used identity lower components sparse provides toeplitz assume consider multivariate next cai recovery recovers mai data characterizes shows population liu", "authors": "Mladen Kolar; Han Liu", "thumbnail_path": "thumbnails/Feature Selection in High-Dimensional Classification.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/kolar13a.pdf", "title": "Markov Network Estimation From Multi-attribute Data", "topics": [0.016958079662238439, 0.016785196332721217, 0.017030805144444584, 0.016820969819091522, 0.016888202718398453, 0.91551674632310576], "most_common_string": "graph matrix distance estimation hamming nodes procedure network covariance data precision partial canonical sample log chain markov blocks results nearest correlation estimate graphical neighbor block estimating set based method given one random model using following gaussian size number selection conditions problem attributes node max diagonal nodal networks however independent update glasso multivariate two attribute also optimization chiquet kolar sparse guo let obtained vectors gene penalized likelihood variables graphs penalty descent yuan structure assume inverse vector consistent current theoretical figure provide paper university note objective multiple learning corresponding proposed step methods single models follow estimates zero new usa lemma known example", "authors": "Mladen Kolar; Han Liu; Eric Xing", "thumbnail_path": "thumbnails/Markov Network Estimation From Multi-attribute Data.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/malioutov13.pdf", "title": "Exact Rule Learning via Boolean Compressed Sensing", "topics": [0.018555130868677688, 0.01818209197576023, 0.01837056804630726, 0.90821824598073819, 0.018430318043921368, 0.018243645084595164], "most_common_string": "rule learning boolean set rules problem compressed data group decision sets sensing testing accuracy via approach exact using sparse matrix training features recovery one also section individual columns optimization proposed samples algorithm combinatorial solution covering results learned relaxation clauses boosting binary interpretability signal interpretable conjunctive use example better approaches number lists single work malyutov conditions error consider linear apply learn thresholds analytics however malioutov property petal heuristic possible nonzero vector second feature paper since programming probability clause cohen min objective continuous show based best terms two simple subjects tests would ixj small learner satisfy table known machine may recover note", "authors": "Dmitry Malioutov; Kush Varshney", "thumbnail_path": "thumbnails/Exact Rule Learning via Boolean Compressed Sensing.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/liu13.pdf", "title": "Sparse Recovery under Linear Transformation", "topics": [0.92563354226737526, 0.014834570291042905, 0.01489847772733718, 0.014858898314546003, 0.014887668401078816, 0.014886842998619627], "most_common_string": "number condition log case matrix random error lasso sparse estimate one probability signal bounded recovery problem gaussian min linear fused analysis consider high measurement transformation two following noisy bound true noiseless let given entries consistent verify paper special assumption section consistency guaranteed constant assume results upper term statistics converges model theorem using sparsity graph theoretical cases since least recover result measurements existing total tao large relative set see learning general value property formulation zhang vector max ieee theory recovered second note edge annals numerical use journal norm solution zero larger work information machine main free standard tibshirani part transactions considered", "authors": "Ji Liu; Lei Yuan; Jieping Ye", "thumbnail_path": "thumbnails/Sparse Recovery under Linear Transformation.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/chen13d.pdf", "title": "Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery", "topics": [0.922993895949071, 0.015369739592410033, 0.015452264010432086, 0.015376498149228675, 0.015410136279047486, 0.015397466019810643], "most_common_string": "support noise algorithm recovery missing results theorem data wainwright knowledge loh performance omp error bounds show regression covariance rosenbaum tsybakov noisy consider two work standard case use log provide independent model method additive proof algorithms selector covariate columns obtain sparse even information matrix guarantees matching setting parameter problem using methods ieee one require note set seems minimax inner simulations theory known following figure gaussian covariates estimate probability improved lower moreover dantzig zit estimator bound correlated assume orthogonal design particular product condition also simple analysis without lasso either let pursuit via control better good gradient projected shows greedy seem grad estimation", "authors": "Yudong Chen; Constantine Caramanis", "thumbnail_path": "thumbnails/Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/ross13b.pdf", "title": "Learning Policies for Contextual Submodular Prediction", "topics": [0.014615048882088246, 0.014574123664231749, 0.014643228445771187, 0.014572347281109474, 0.92695557762980874, 0.014639674096990659], "most_common_string": "list learning submodular online policies scp loss features policy contextual algorithm performance prediction using training item distribution optimization items approach use set state problem learner let weighted reduction sequence construct best guarantees dey data predict greedy conseqopt lists function convex denote update cti work example one lin user also setting regret streeter guestrin good yue min expected thus regression document reward examples functions consider golovin goal may value probability directly class result clairvoyant initial single test section bilmes agnostic learn vti linear bagnell length multiple rouge maximize trajectory case wti trajectories position via often submodularity problems joachims taskar new pick", "authors": "Stephane Ross; Jiaji Zhou; Yisong Yue; Debadeepta Dey; Drew Bagnell", "thumbnail_path": "thumbnails/Learning Policies for Contextual Submodular Prediction.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/golub13.pdf", "title": "Learning an Internal Dynamics Model from Control Demonstration", "topics": [0.023192147564378915, 0.023091240926835774, 0.023212456273313174, 0.02311600909072031, 0.023255933226017701, 0.88413221291873412], "most_common_string": "internal model cursor control plant state dynamics bmi ime feedback subject timestep actual learning estimates may target position cost neural estimate current data aiming function trial delay parameters sensory latent motor estimation visual task demonstration true problem models section states framework angular drive across graphical work training given trajectory straight error goals likelihood machine optimal abbeel trajectories variables probabilistic international activity commands recorded algorithm inverse errors figure belief system note knowledge available due timesteps seek input predictions form line time applied demonstrations along use would mismatch known schwartz previously extract red toward access black observed see points movements signals using", "authors": "Matthew Golub; Steven Chase; Byron Yu", "thumbnail_path": "thumbnails/Learning an Internal Dynamics Model from Control Demonstration.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/pirotta13.pdf", "title": "Safe Policy Iteration", "topics": [0.018580720239619531, 0.018597603505546515, 0.90696319137911796, 0.018581070342463864, 0.018643285276476785, 0.018634129256775236], "most_common_string": "policy state iteration improvement algorithms bound policies two value following one function uspi mspi safe cpi greedy performance iterations amspi kakade algorithm auspi states advantage number approximate corollary theorem distribution consider values optimal since figure matrix using starting set case step chain may paper target proposed approximation current spi improving approaches new taking section derivative langford transition given bounds action lower stationary acpi api approximated maximizes gradient estimate size parr future proceedings lemma factor approach expected larger guaranteed blackjack follows convex discount initial whose worse version use better return domain space card considered problem perform discounted model reward learning possible", "authors": "Matteo Pirotta; Marcello Restelli; Alessio Pecorino; Daniele Calandriello", "thumbnail_path": "thumbnails/Safe Policy Iteration.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/tamar13.pdf", "title": "Temporal Difference Methods for the Variance of the Reward To Go", "topics": [0.014430885259829596, 0.014362126970243983, 0.014499586551631718, 0.014411487713465714, 0.014406509281522777, 0.92788940422330624], "most_common_string": "variance may state approximation equation policy let function reward algorithm bertsekas lstd denote proposition solution projected evaluation methods linear point learning following assumption using norm second matrix vector value algorithms space standard show based exists trajectories convergence temporal lemma reinforcement barto large mdp also sutton respect thus guarantees unique decision deviation consider stochastic section onto features given result approach estimation moment projection obtained positive known process assumptions propose framework inequality weighted estimate probability continuous terminal mannor theorem step approximate distribution markov version estimates even method equations domains next therefore criteria appendix states proof control figure machine tsitsiklis mapping used denotes", "authors": "Aviv Tamar; Dotan Di Castro; Shie Mannor", "thumbnail_path": "thumbnails/Temporal Difference Methods for the Variance of the Reward To Go.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/brechtel13.pdf", "title": "Value Iteration with incremental representation learning for continuous POMDPs", "topics": [0.015018590090708155, 0.015017461670266748, 0.92482769896859884, 0.015038884929592719, 0.015067216905973166, 0.015030147434860454], "most_common_string": "value continuous representation belief discrete space backup iteration state learning bound pomdps beliefs problem pomdp function every solving lower upper optimal using decision thus policy samples results presented planning process solver implementation porta observations problems algorithm represent concept states incremental observation set obstacle case must poupart particles approaches also new solve step idea intelligence time linear even represented agent end corridor number tree used sparse operator bounds functions monte approximation evaluated carlo values machine robot research smc one need expectation observable according systems procedure reward intersection arbitrary journal basis approximate partially general novel second use next therefore known information shows", "authors": "Sebastian Brechtel; Tobias Gindele; R diger Dillmann", "thumbnail_path": "thumbnails/Value Iteration with incremental representation learning for continuous POMDPs.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/lattimore13.pdf", "title": "The Sample-Complexity of General Reinforcement Learning", "topics": [0.01863710924910927, 0.018553111999670104, 0.018634535739708625, 0.01858611710245103, 0.90691294287836388, 0.018676183030696972], "most_common_string": "exploration learning phase environments environment merl bound emax lemma let number bounds reinforcement general since policy phases set gmax algorithm class case probability hutter optimal log true history model failure lower theorem classes proper follows start therefore high show mdps proof given also exists lattimore union upper work proceedings conference value used exploiting statistics results international known rather machine one respect sequence time compact finally martingale regret end return least nearly state policies discounted reward either function apply probabilities values example write complexity algorithms large see close would expected without strehl arbitrary within samplecomplexity problem maximum bandits never explore logarithmic", "authors": "Tor Lattimore; Marcus Hutter; Peter Sunehag", "thumbnail_path": "thumbnails/The Sample-Complexity of General Reinforcement Learning.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/nguyen13.pdf", "title": "Online Feature Selection for Model-based Reinforcement Learning", "topics": [0.92531839246866976, 0.014855896390299944, 0.015094657082827692, 0.014837419436085467, 0.01492494851655086, 0.014968686105566351], "most_common_string": "learning state transition features feature model online lorerl regression optimal action logistic selection reinforcement frmax reward function factored also method multinomial however dynamics proceedings fepsg time environment learn states algorithm may policy small figure dbn data mdp based relevant conference structure agent machine matrix space rmax robot average exploration let actions accumulated parameter cmdp world models large regret binary vector random using stone episodes still group number structures mdagl set irrelevant value near manually used task sparse four international work table information regularization framework since probability without knowledge parameters experiments capture changes lasso often regularized predicting methods blorerl michael selected", "authors": "Trung Nguyen; Zhuoru Li; Tomi Silander; Tze Yun Leong", "thumbnail_path": "thumbnails/Online Feature Selection for Model-based Reinforcement Learning.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/bellemare13.pdf", "title": "Bayesian Learning of Recursively Factored Environments", "topics": [0.015827115652539508, 0.015813229489958128, 0.015905850180916021, 0.015837750814859341, 0.015962902216806973, 0.92065315164492001], "most_common_string": "model environment learning space recursively factored factorization reinforcement set factorizations bayesian observation models decomposable veness atari qtf used given dim conference using nesting prior number best averaging possible time equation depth example base spaces intelligence log bellemare tree class patch environments image large section percept information recursive silver size factor algorithm joel cts domains notation many context denote hutter factors patches technique string larger product introduce denoted one probabilistic techniques case action also game loss describe whose international allows two particular weighting machine algorithms paper bowling performance frame approach games general neural processing within frames corresponding results planning sequential logarithmic", "authors": "Marc Bellemare; Joel Veness; Michael Bowling", "thumbnail_path": "thumbnails/Bayesian Learning of Recursively Factored Environments.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/friedland13.pdf", "title": "Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events", "topics": [0.92461476881472426, 0.015028453535508281, 0.015128427823132579, 0.015048419818492042, 0.015121230700228871, 0.015058699307914031], "most_common_string": "pairs data model cij number social positive one pair performance section similarity points score ratio using likelihood task dij distance set sets distributions auc twins method inference figure negative generative mij function true events detecting also two distribution would generate copy normal duplication entities point coincidence increases parameters values process parameter measure since could mining rarity detection almost measures form synthetic shows phone information ranking experiments baseline national estimate use science work links may lines paper matches always singleton variables instances given individual linked real know methods similar generated jensen turns large small contains entity ieee list network probability mixture", "authors": "Lisa Friedland; David Jensen; Michael Lavine", "thumbnail_path": "thumbnails/Copy or Coincidence? A Model for Detecting Social Influence and Duplication Events.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13a.pdf", "title": "Mixture of Mutually Exciting Processes for Viral Diffusion", "topics": [0.0174475120900019, 0.017412571187049235, 0.017606742441291151, 0.017414028500206648, 0.91254261476960885, 0.017576531011842166], "most_common_string": "network meme model inference viral memes events mixture algorithm process one models event time exciting mutually hawkes social two processes set point variational figure data hidden content log node rate use number note also modeling tracking fast nodes future based infected prior example inferred intensity snowsill using netrate simultaneously latent work history infectivity algorithms evolution probabilistic mhps mhp twitter cascades single structure matrix results tasks mmhp another shows contents multiple experiments kernel learning sparsity assuming comparison sequence function true existing large current causality parameters usually known rmse challenges used trends variables given elbo blogosphere involving terms genetic dag independent identify", "authors": "Shuang-Hong Yang; Hongyuan Zha", "thumbnail_path": "thumbnails/Mixture of Mutually Exciting Processes for Viral Diffusion.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/heaukulani13.pdf", "title": "Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks", "topics": [0.017038563674448944, 0.017025139831178187, 0.91477388228848366, 0.017025974217395542, 0.017087714581835934, 0.017048725406657774], "most_common_string": "latent feature social network model time hik data propagation networks features dataset dynamic markov nips lfp drift models see lfrm state parameters also distribution probability given using actor space variables structure current two following representations baseline one relational observed infocom use matrix observations set future samples link ghahramani results auc particular work parameter transition however previous methods prediction states number section statistically forecasting test method inference order hidden bernoulli training authors point snijders xing based foulds datasets chain miller prior sample example links information used mcmc interests probabilistic modelling hmm perform edges sequence year stochastic large interactions unobserved figure next", "authors": "Creighton Heaukulani; Ghahramani Zoubin", "thumbnail_path": "thumbnails/Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/gomez-rodriguez13.pdf", "title": "Modeling Information Propagation with Survival Theory", "topics": [0.017978159118249019, 0.017951063252632238, 0.91003834325544497, 0.017974206528506498, 0.01807326075841385, 0.017984967086753487], "most_common_string": "node cascade model nodes additive network information infected multiplicative time hazard infection models set propagation risk cascades test inference using likelihood consider networks survival function rate edge parameters times also contagion previously one leskovec observation proceedings sets infer theory accuracy modeling conference window given shaping methods mse therefore increase number size ccdf international independent edges duration two general optimal data performance several underlying memes log however learning acm vector decrease negative wang process parameter term used show observed real evaluate compute problem functions generated synthetic convexity solution knowledge apply training particular example machine synthetically since first mining continuous random sites", "authors": "Manuel Gomez-Rodriguez; Jure Leskovec; Bernhard Schlkopf", "thumbnail_path": "thumbnails/Modeling Information Propagation with Survival Theory.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/zhou13.pdf", "title": "Learning Triggering Kernels for Multi-dimensional Hawkes Processes", "topics": [0.020685695149050897, 0.020610849810372967, 0.020789203735415158, 0.020640299739052986, 0.020798658853487501, 0.89647529271262061], "most_common_string": "kernels triggering kernel data hawkes base performance processes mmel figure learning true estimated loglik proposed events equation number problem method respect function parameters process set training algorithm follows work used measured smoothing nonparametric estimate dimensional exponential model samples two log time synthetic plot use estmated observed observe journal test dynamics particular better related linear quite intensity show thus real ijd moreover general models section optimizing following machine parameter paper propose splines constraints generate another also auuc based experiments case value optimization sets dimension world point functional distribution consider social positive values information end temporal event large relatively lewis estimation trigger", "authors": "Ke Zhou; Le Song; Hongyuan Zha", "thumbnail_path": "thumbnails/Learning Triggering Kernels for Multi-dimensional Hawkes Processes.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/herlau13.pdf", "title": "Modeling Temporal Evolution and Multiscale Structure in Networks", "topics": [0.014818388866950119, 0.92576956008777789, 0.014941843043141766, 0.014778835587655305, 0.014895373958851576, 0.014795998455623153], "most_common_string": "temporal model vertices networks network tree structure hierarchical epoch vertex models change giant two using multiscale relational evolution epochs gibbs states irm modeling time democrats republicans fragmentation schmidt leafs set hierarchies consider thrm number see organization nips changes hrm complex shown issn considered information senators hierarchy jit distributed present right prior data onto voting trees new according modelling moves edge proposed senate results matrix construction enron binary may systems one given beta aij edges subtree unique methods bottom properties mccullagh single dynamic roy auc indicating let social type science process processes level probability illustrate top observations inferred multifurcating corresponds community", "authors": "Tue Herlau; Morten Mrup; Mikkel Schmidt", "thumbnail_path": "thumbnails/Modeling Temporal Evolution and Multiscale Structure in Networks.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/yang13b.pdf", "title": "Scalable Optimization of Neighbor Embedding for Visualization", "topics": [0.016009770420972701, 0.01595812631578419, 0.016084111142763386, 0.015973968620135004, 0.01602457392445189, 0.91994944957589286], "most_common_string": "data methods embedding neighbor points visualization approximated fast optimization approximation learning cost large exact gradient objective method mnist using subset qij time computation information scalable sets pij hinton tree computed point classes function even figure machine algorithms new approach algorithm small hours sne size one group computational also der relative conference van maaten datasets compared original results stochastic bounding shows international box larger example complexity experiments systems uci mean matrix space linear timit neighborhood quality set whole nerv nldr several well samples output node images log approximate structure technique subsampling smaller nonlinear contains two covertype shown dataset divergence another pairwise", "authors": "Zhirong Yang; Jaakko Peltonen; Samuel Kaski", "thumbnail_path": "thumbnails/Scalable Optimization of Neighbor Embedding for Visualization.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/gens13.pdf", "title": "Learning the Structure of Sum-Product Networks", "topics": [0.018048608316768227, 0.017956748949328149, 0.90991367612122809, 0.017978270595216728, 0.018056966042591913, 0.018045729974866979], "most_common_string": "learning variables spn spns structure networks number instances query algorithm learnspn set learned evidence inference sum markov partition domingos graphical datasets models subsets test function distribution cll node variable time likelihood also used univariate using nodes network probability returns instance independent model pll product independence winmine davis computed bayesian cluster split lowd weights recursive xij return similar distributions wij clusters weight linear comparable table step fraction poon advantage splitting large let normalized root sij data found since mixture tractable many sampling corresponding case splits value della proposed leaf pietra dataset conditional deep compared results webkb gens use dna two values", "authors": "Robert Gens; Domingos Pedro", "thumbnail_path": "thumbnails/Learning the Structure of Sum-Product Networks.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/coates13.pdf", "title": "Deep learning with COTS HPC systems", "topics": [0.015345778958687347, 0.01531522745895364, 0.92324291883578513, 0.015355528693961557, 0.015378663895123153, 0.015361882157489258], "most_common_string": "gpu gpus learning deep network neural systems large networks neurons figure use training using billion code size system train layer input computation parameters many hpc array receptive computing found krizhevsky implementation neuron cluster images number gradient cots much responses used local image compute larger pooling parameter also distributed results layers international blocks shown though optimized set infrastructure conference make dean scale processing communication single mpi contrast must one best lecun several output recognition block software trained memory information machine inputs paper window linear features normalization approach communications sparse coates matrix section obtained hinton convolutional scaling need able performance nvidia unsupervised", "authors": "Adam Coates; Brody Huval; Tao Wang; David Wu; Bryan Catanzaro; Ng Andrew", "thumbnail_path": "thumbnails/Deep learning with COTS HPC systems.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/sohn13.pdf", "title": "Learning and Selecting Features Jointly with Point-wise Gated Boltzmann Machines", "topics": [0.017443434261426586, 0.017361690937544465, 0.017697499489437635, 0.017552391826182342, 0.017561601396672542, 0.91238338208873648], "most_common_string": "units pgbm hidden features learning feature visible model switch unit using data object boltzmann selection rbm supervised training irrelevant patterns deep two raw images learn generative used gated section component layer background convolutional figure group bengio models machines machine propose bounding lee network mixture neural performance jointly hinton foreground class larochelle cpgdn components follows second use unsupervised corresponding unlabeled recognition selecting large activations representations given one table accuracy label useful algorithm method groups imrbm sohn perform complex robust discriminative building learned train example may dataset shown shows show joint however box conditional examples block icml also inference distribution caltech test", "authors": "Kihyuk Sohn; Guanyu Zhou; Chansoo Lee; Honglak Lee", "thumbnail_path": "thumbnails/Learning and Selecting Features Jointly with Point-wise Gated Boltzmann Machines.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/wan13.pdf", "title": "Regularization of Neural Networks using DropConnect", "topics": [0.023241137359203481, 0.023074452632132796, 0.023280095085904912, 0.88414027892139813, 0.023201772905235639, 0.023062263096125014], "most_common_string": "dropconnect dropout layer network training feature model using error fully connected networks neural output mask input function images extractor weight rate learning matrix activation performance parameters table hinton softmax gpu set test weights regularization complexity voting random layers data train size lemma results example previous loss use relu generalization show rademacher units applied krizhevsky large activations unit memory experiments implementation ciresan mnist methods bound initial models convolutional rather zeiler features number result used epochs image single elements section experiment fergus inference trained element dataset mean shows randomly computer described tanh lecun biases functions consider two equation ieee bernoulli since standard", "authors": "Li Wan; Matthew Zeiler; Sixin Zhang; Yann Le Cun; Rob Fergus", "thumbnail_path": "thumbnails/Regularization of Neural Networks using DropConnect.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/tran13.pdf", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "topics": [0.014110078598141991, 0.01407458789266707, 0.014139065050517936, 0.014120309073456417, 0.92946368941965096, 0.014092269965565608], "most_common_string": "boltzmann tbm data variables gaussian hidden constraints model machines binary one learning thus distribution thurstonian using categories rbm probit ordinal evidences statistics information types rank continuous set collaborative pages evidence case analysis models section salakhutdinov machine inequality used point samples standard observed see underlying layer per survey hinton particular figure categorical probability units without input boxed ranking processing subset ordered multivariate unit two applications truyen matrix posteriors restricted given need proceedings inequalities however simple inference normal due chain markov example parameter max tran category since xil countries constrained min multiple latent learned best user boundaries many much systems general neural", "authors": "Truyen Tran; Dinh Phung; Svetha Venkatesh", "thumbnail_path": "thumbnails/Thurstonian Boltzmann Machines: Learning from Multiple Inequalities.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/karbasi13.pdf", "title": "Iterative Learning and Denoising in Convolutional Neural Associative Memories", "topics": [0.01430297842399585, 0.01422974949890334, 0.014342168393065412, 0.014254417349190168, 0.92860924780724785, 0.014261438527597417], "most_common_string": "learning algorithm neural pattern patterns set network capacity recall associative error cluster noise one phase neurons degree convolutional iterative number graph denoising work ieee proposed input constraint vectors figure constraints also noisy single results linear among neuron theorem memories vector learn algorithms correct retrieval size given sparsity matrix orthogonal entries small training iteration networks local memorized performance fraction large shown note threshold state subspace dual let dbns redundancy weight probability bipartite show oja case exponential model errors section kumar following proof order clusters furthermore features correction able new considered based lausanne used overlapping similar salavati assume since consider nodes method", "authors": "Amin Karbasi; Amir Hesam Salavati; Amin Shokrollahi,", "thumbnail_path": "thumbnails/Iterative Learning and Denoising in Convolutional Neural Associative Memories.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/schaul13.pdf", "title": "No more pesky learning rates", "topics": [0.01595227367308294, 0.92060623362800464, 0.015975477047000069, 0.015822977074266404, 0.015848339898575915, 0.015794698679070164], "most_common_string": "learning rate sgd loss rates gradient adaptive one parameter diagonal training method optimal hessian error algorithm stochastic neural parameters best lecun estimates table samples test hidden performance tuning using quadratic update global pesky curvature adagrad used case bottou almeida figure machine algorithms smd number note average local settings set problem variance vsgd expected value amari methods schedule two sample data formula small given need mnist optimum across terms online layer see supplementary network time layers large approximation reconstruction use compare natural better denoted function new approach changes scale automatically dataset approximate form distribution deep single benchmark results norm every updates", "authors": "Tom Schaul; Sixin Zhang; Yann LeCun", "thumbnail_path": "thumbnails/No more pesky learning rates.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/bergstra13.pdf", "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures", "topics": [0.016534369474162335, 0.016513123851927047, 0.016715614352930428, 0.016608778696252015, 0.91708327674202417, 0.016544836882704052], "most_common_string": "search model optimization random hyperparameter tpe algorithm pinto bergstra approach cox used vision data performance work best image hyperparameters set within making algorithms found space computer models science feature many learning loss images recognition trials svm test machine lfw features one class components function pooling two figure null error parameters view bayesian value coates spatial results parameter bank training language object experiments automatic lnorm expression choose task using tuning validation include values research given system face fbncc new distribution thousand neural hutter carried choice uniform particular history dihist better settings gaussian operation approaches typically distributed local prior patch three applied", "authors": "James Bergstra; Daniel Yamins; David Cox", "thumbnail_path": "thumbnails/Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/dalalyan13.pdf", "title": "Learning Heteroscedastic Models by Convex Programming under Group Sparsity", "topics": [0.014664357738398253, 0.014491974677587905, 0.92724712420312982, 0.01452060020849591, 0.014541482609467225, 0.0145344605629207], "most_common_string": "scheds group convex one lasso sparsity noise functions vector heteroscedastic assumption estimation procedure regression linear variance conditional function mean methods log programming learning model dantzig problem risk let theoretical estimating set theorem sparse models matrix case time large method scaled two selector new tuning optimization level constraints may result well also estimator bounded results covariates zhang prediction temperatures standard values data order used unknown optimal point proposed bounds constant paris every even use additive assumptions parameter propose gaussian since card number given std ofo interior sun based means joint terms bias dimensional denote ave temperature using condition diag obtained parameters", "authors": "Arnak Dalalyan; Mohamed Hebiri; Katia Meziani; Joseph Salmon", "thumbnail_path": "thumbnails/Learning Heteroscedastic Models by Convex Programming under Group Sparsity.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/wang13.pdf", "title": "Noisy Sparse Subspace Clustering", "topics": [0.016791697496893982, 0.016569025825357644, 0.91675993748083284, 0.01658276585251935, 0.016663030613531821, 0.016633542730864329], "most_common_string": "subspace noise clustering data noisy ssc random theorem analysis subspaces sparse log figure property matrix dual model problem min vidal detection soltanolkotabi lasso results solution ieee rank candes remark max points geometric let inradius deterministic proof magnitude lemma samples range algorithm even guarantee number relviolation dimension convex bound robustness requires two recovery machine motion spectral columns set sep see point margin incoherence elhamifar constant case projection transactions holds exact furthermore conditions error singapore robust supplementary practical also denote uniformly consider condition noiseless vector version increasing method segmentation theoretical following paper perfect optimal fully gaussian projected note face trivial intelligence drawn", "authors": "Yu-Xiang Wang; Huan Xu", "thumbnail_path": "thumbnails/Noisy Sparse Subspace Clustering.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/gopi13.pdf", "title": "One-Bit Compressed Sensing: Provable Support and Vector Recovery", "topics": [0.017728713569035132, 0.017433995334689929, 0.017488188149970042, 0.017476013685175516, 0.91241145894040721, 0.017461630320722048], "most_common_string": "algorithm recovery support log using sensing measurements matrix measurement vector compressed sign recover problem theorem approximate uff number based set methods section probability note plan see signal sparse baraniuk vershynin given design provable algorithms also compressive method error universal existing let following twostage supplementary sets expanders results matrices provide material denotes linear standard however present several robust signals output two supp time solution figure complexity ieee expander yes input constructed elements large vectors family high end free random approach union well underlying stage next parameters use haupt known tao information better optimal used transactions richard laska learning larger propose varying", "authors": "Sivakant Gopi; Praneeth Netrapalli; Prateek Jain; Aditya Nori", "thumbnail_path": "thumbnails/One-Bit Compressed Sensing: Provable Support and Vector Recovery.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/balasubramanian13.pdf", "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations", "topics": [0.019133414684814758, 0.019049527580633194, 0.90459208131389812, 0.019056146962408722, 0.019089486263417162, 0.019079343194828333], "most_common_string": "sparse coding dictionary smooth regression learning marginal data using standard kernel approach based set proposed step codes method lasso error sample feature used function representations accuracy local theorem reconstruction update algorithm similarity use image problem one see corresponds samples table space experiments optimization convergence time norm two following example complexity propose gradient could features alternative recognition smoothing bounds results statistical matrix size also advantage projection information may temporal might training generalization min respect corresponding several leads better main framework term related given rates distance note class traditional speedup covering score previous procedure descent obtain bandwidth comparison videos functions test vector", "authors": "Krishnakumar Balasubramanian; Kai Yu; Guy Lebanon", "thumbnail_path": "thumbnails/Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/kyrillidis13.pdf", "title": "Sparse projections onto the simplex", "topics": [0.014371862440054922, 0.01419272516991619, 0.92875973807424383, 0.014213594877485638, 0.014251945001027427, 0.014210134437271977], "most_common_string": "approach problem convex algorithm sparse onto solution let simplex true since projections estimated rank kernel measurements projection figure given pdf means matrix portfolio set quantum number also learning tomography constraints density support vector via use following hence relative using constraint index projector recovery one gradient approaches solutions case loss parzen sparsity argmin assume results cardinality result based liu euclidean optimization error rip function supp time approximation proof sample element gaussian shows greedy probability gssp trace estimation thus minimize nonconvex obtain guarantees quadratic cevher kyrillidis consider norm even hyperplane order eigenvalue operator restricted note random gshp knowledge experiments argmax avg descent", "authors": "Anastasios Kyrillidis; Stephen Becker; Volkan Cevher; Christoph Koch", "thumbnail_path": "thumbnails/Sparse projections onto the simplex.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/richard13.pdf", "title": "Intersecting singularities for multi-structured estimation", "topics": [0.9237671794215927, 0.015195709158065191, 0.015291710147438164, 0.015237410397401402, 0.015285133674313476, 0.015222857201189275], "most_common_string": "norm rank sparse trace matrix estimation convex matrices singularities using penalty linear ranksity case noise vec two intersecting regularizers span log optimization block problem algorithm following new function index singular lifting algorithms supp consider lifted one given point dimension orthogonal penalties chandrasekaran penalized cases min points set see value learning regularizer dense values diag component objects theoretical space robust normal sum instance operator results level respectively compressed subgradient nonsmooth least observation norms subspaces experiments cone used analysis sensing sparsity inf entries dim unit use call information vandergheynst machine estimating structural denotes onto multiple bound state written tuning numerical intersection bach", "authors": "Emile Richard; Francis BACH; Jean-Philippe Vert", "thumbnail_path": "thumbnails/Intersecting singularities for multi-structured estimation.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/zhang13.pdf", "title": "Sparse Uncorrelated Linear Discriminant Analysis", "topics": [0.01977368575881296, 0.019656135685750305, 0.019778907481157508, 0.90133432852911788, 0.01974219786900086, 0.019714744676160727], "most_common_string": "sparse data lda discriminant solution sulda ulda class problem analysis linear uncorrelated optimization trace solutions slda orthogonal transformation minimum matrix generalized plda dimension sparsity rank bregman results number space scatter optimal linearized features extracted column algorithm accelerated algorithms yin journal let two tibshirani computed characterization theorem classical matrices vector gene statistical based method arg using section selected chu applications machine denotes training table accuracy set learning mutually compute implies cai max solving fukunaga one paper satisfying however huang many experimental thus reduced hastie srbct existing solved international test following feature experiments jin since projected directly classes null min also penalty", "authors": "Xiaowei Zhang; Delin Chu", "thumbnail_path": "thumbnails/Sparse Uncorrelated Linear Discriminant Analysis.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/lopes13.pdf", "title": "Estimating Unknown Sparsity in Compressed Sensing", "topics": [0.93161664705352443, 0.013644110595485263, 0.013734057652911583, 0.01365840774854425, 0.013695423311641803, 0.013651353637892842], "most_common_string": "sparsity measurements estimating estimate error rank section matrix unknown recovery problem value parameter relative signal stable compressed ieee also vectors sensing theorem bound log number random set measurement theory choice procedure deterministic small reconstruction order since show estimation noise gaussian case coordinate using sparse measure matrices level large vector based known figure transactions coordinates note distribution linear let parameters high values assumptions given one result many well dimension left signals estimator function depend standard two theoretical information may shows computed property used sets right scale additional true journal important assume interval results entries approximation estimated following obtained plotted use cauchy", "authors": "Miles Lopes", "thumbnail_path": "thumbnails/Estimating Unknown Sparsity in Compressed Sensing.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/silver13.pdf", "title": "Concurrent Reinforcement Learning from Customer Interaction Sequences", "topics": [0.019317206584970818, 0.019315221564513447, 0.019438104605523431, 0.01928540442045628, 0.019408193070535033, 0.90323586975400094], "most_common_string": "customer learning concurrent reinforcement interaction customers interactions algorithm online actions time updates function real example concurrency company may algorithms action variables null bandit decision contextual decisions one email given sutton however many sequences policy using learn history data response observations rewards applied batch value update environment sequential performance interacting reward sequence scenarios agent work occur parallel internet also measured simulator setting abe prior two optimal event note pages distributed conference partial large new figure high times framework typically international subsequent requests used levels opportunity problem single compared requested options total therefore feature might number immediate observation request histories state approach", "authors": "David Silver; Leonard Newnham; David Barker; Suzanne Weller;Jason McFall", "thumbnail_path": "thumbnails/Concurrent Reinforcement Learning from Customer Interaction Sequences.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/hamilton13.pdf", "title": "Modelling Sparse Dynamical Systems with Compressed Predictive State Representations", "topics": [0.015889596853960217, 0.015842917937344304, 0.015916030583452454, 0.015831837761489453, 0.92066650120276616, 0.015853115660987366], "most_common_string": "cpsr algorithm tests compressed learning systems observation model error tpsr large prediction set matrices used matrix state dynamical models space probability one also observable using boots projection vector random sparse domain number history algorithms histories thus dimension psr empirical domains predictive however results estimates would sample observations bound modelling possible mol psrs spectral method work representation compression gordon regression analysis partially use probabilities distribution figure information representations size able pocman variance theorem present test assume include learned therefore reduce since linear target projections approach estimation col sparsity veness column case let parameters particular feature shows length maillard singh spaces many", "authors": "William Hamilton; Mahdi Milani Fard,; Joelle Pineau,", "thumbnail_path": "thumbnails/Modelling Sparse Dynamical Systems with Compressed Predictive State Representations.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/sodomka13.pdf", "title": "Coco-Q: Learning in Stochastic Games with Side Payments", "topics": [0.019530415251145461, 0.01961402998237731, 0.019694745466922781, 0.90181930404993915, 0.019662449380934914, 0.019679055868680132], "most_common_string": "coco values game games goal figure side learning stochastic value agents trajectory grid payments step two agent ego move minmax one operator players kalai left maxmax nash policy probability friend reach set shared solution possible time alter joint goals littman action proceedings player would machine square shown conference reward algorithm play equilibrium actions expected also turkey convergence moves incredible foe michael sum international since converges converge state unique right moving pays transfer concept team policies total use operators example states greenwald shows illustrate note without payment solutions symmetric however second corresponding prisoner made sticks functions correlated stick markov equation following", "authors": "Elizabeth Hilliard; Eric Sodomka; Michael Littman; Amy Greenwald", "thumbnail_path": "thumbnails/Coco-Q: Learning in Stochastic Games with Side Payments.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/levine13.pdf", "title": "Guided Policy Search", "topics": [0.017534708336205958, 0.017535707035720681, 0.017623022983541119, 0.01752521805605398, 0.91221719664364997, 0.017564146944828253], "most_common_string": "policy samples guiding ddp search learning initial methods used tbdp gps policies also reward guided method use trajectory using distributions prior importance distribution optimization neural example new gradient test sample algorithm current learn optimal systems international conference figure since sampling rollouts learned work adaptive variant local line states abbeel good mean gaussian iteration show complex actions previous humanoid examples log making high hidden shown could walking set often estimator dynamic stochastic section approach regions one given exp state tang well peters low reinforcement single walker suitable machine return term found trajectories standard training require hopper schaal best units terrains sampled", "authors": "Sergey Levine; Vladlen Koltun", "thumbnail_path": "thumbnails/Guided Policy Search.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/goschin13.pdf", "title": "The Cross-Entropy Method Optimizes for Quantiles", "topics": [0.014182189049766356, 0.92901728536065364, 0.014233909654231398, 0.014175537442352048, 0.014198756062333543, 0.01419232243066307], "most_common_string": "proportional distribution policy algorithm value method mce quantile quantiles convergence values optimization generation optimizes set algorithms results two expected reward one policies input noise distributions setting parameters optimal standard szita rubinstein search inputs evaluations space rho solutions game tetris function number experiment mannor samples noisy particular according learning initial maximum properties control stock profile goal paper also performance case problem empirical thus bernoulli used blackjack using goschin stochastic section version example machine operations kroese reasonable research stage population determined ran show experiments state various simple inventory well repeated time second even theoretical parameter variant executed weight ordering selection phenomenon converge", "authors": "Sergiu Goschin; Ari Weinstein; Michael Littman", "thumbnail_path": "thumbnails/The Cross-Entropy Method Optimizes for Quantiles.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/arora13.pdf", "title": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "topics": [0.91933296624053173, 0.016106259693781766, 0.016177138971574658, 0.016081836596777095, 0.016214614711748482, 0.016087183785586476], "most_common_string": "algorithm topic anchor matrix algorithms documents words topics recover error word model arora gibbs data recovery points set rows provable using use recoverkl models results given vertices new modeling document guarantees distributions nips linear sampling step synthetic parameters corpora practical found number blei point learning hull time dirichlet times corpus close distribution coherence convex shown running probability zzz simplex whose even mccallum two work one performance selection many present sample david mcmc likelihood separability show based mimno empirical zero used analysis solve lda supplementary log generated large learned method let latent least also three procedure input real span vertex row", "authors": "Sanjeev Arora; Rong Ge; Yonatan Halpern; David Mimno; Ankur Moitra; David Sontag; Yichen Wu; Michael Zhu", "thumbnail_path": "thumbnails/A Practical Algorithm for Topic Modeling with Provable Guarantees.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/zhai13.pdf", "title": "Online Latent Dirichlet Allocation with Infinite Vocabulary", "topics": [0.017319428457837176, 0.91333129030521876, 0.017369614318706927, 0.017234171760331832, 0.017445120075815779, 0.017300375082089602], "most_common_string": "topic words distribution models model online vocabulary infvoc dirichlet variational blei inference word topics minibatch latent pmi truncation process better tos settings section documents parameters use distributions algorithms set bayesian allocation figure parameter base david score wang nonparametric new however learning coherence lda scale newsgroups reordering probability dtm two delay used language jordan approach modeling document streaming minibatches algorithm consider large zdn rank vocabularies later capture higher uses possible dynamic stochastic index instead size corpus accuracy values mimno batch underlying within conditional hybrid strings level multinomial based approaches contains must information hierarchical data allow choose reasonable log training length number", "authors": "KE ZHAI; Jordan Boyd-Graber", "thumbnail_path": "thumbnails/Online Latent Dirichlet Allocation with Infinite Vocabulary.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/zhu13.pdf", "title": "Gibbs Max-Margin Topic Models with Fast Sampling Algorithms", "topics": [0.9155009925590023, 0.016831569235075842, 0.016918365139484432, 0.016870685941434184, 0.017017006740040416, 0.016861380384962963], "most_common_string": "gibbs topic medlda distribution data posterior expected sampling training max gibbsmedlda model time set loss models exp using learning algorithms problem binary topics accuracy zhu prediction supervised performance margin regression vmedlda solve lda svm methods fast variational collapsed min also latent see inference term multiple lemma testing augmentation results sample machine gaussian zdn standard one given seconds draw existing large prior research figure assumptions shows estimate variables number need words experiments bayesian document uses hinge conditional inverse assignments vector upper word reviews learn numbers information response jiang since likelihood dirichlet maximum restricting due new used without scale journal distributions log", "authors": "Jun Zhu; Ning Chen; Hugh Perkins; Bo Zhang", "thumbnail_path": "thumbnails/Gibbs Max-Margin Topic Models with Fast Sampling Algorithms.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/shalit13.pdf", "title": "Modeling Musical Influence with Topic Models", "topics": [0.019586919242938126, 0.019606943821470477, 0.019596980608058076, 0.019587888073434916, 0.90205560006265928, 0.01956566819143914], "most_common_string": "songs topic model musical rock song music jazz innovation artists models modeling using dataset metal time blei blues artist audio features classic later scores use hop ranked one hip international proceedings pop genres folk information years topics genre epoch used figure data gerrish measure median conference epochs popular year structure two spearman score several found dim indie earlier innovative number mean content electronic overall evolution retrieval approach many learning baseline early distribution acoustic since rap considered across described much funk study ismir tags known soul correlation research shows million language rank given top new available examples table single results include", "authors": "Uri Shalit; Daphna Weinshall; Gal Chechik", "thumbnail_path": "thumbnails/Modeling Musical Influence with Topic Models.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/ahmed13.pdf", "title": "Nested Chinese Restaurant Franchise Process: Applications to User Tracking and Document Modeling", "topics": [0.014297402920525212, 0.014366702813997627, 0.92842530411517421, 0.014260261664664398, 0.014360088868980903, 0.014290239616657636], "most_common_string": "model process node hierarchical tree restaurant distribution chinese location user topics nested sampling structure topic language child global franchise table models new modeling document path words algorithm use sample probability regions dirichlet set using moreover data assume inference tweets hong documents two since given ncrf tweet exact root eisenstein content variables number microblogs ahmed users roj results nips hpam blei terms methods nodes regional dir teh usa figure hierarchy generative paths generating thus used ncrp object vertex dataset associated distributions test xoj method full represented want rather one need note best approach component however region obtain twitter zoj existing pachinko", "authors": "Amr Ahmed; Liangjie Hong; Alexander Smola", "thumbnail_path": "thumbnails/Nested Chinese Restaurant Franchise Process: Applications to User Tracking and Document Modeling.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/williamson13.pdf", "title": "Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models", "topics": [0.016072581415978931, 0.016001176987610419, 0.016126130266982609, 0.016011288381209521, 0.91973631326295735, 0.016052509685261111], "most_common_string": "dirichlet data inference cluster process processor time parallel using models mixture hdp model number gibbs processors monte carlo nonparametric allocations distribution sampling auxiliary clusters one variational sampler methods points distributed local global markov avparallel random used approximate single algorithm chain teh described set conditional gamma variable algorithms synch independence given according steps existing method obtained figure probability observations measures xmi approach conditioned perform split perplexity implemented sequential introducing representation since quality performance eight point bayesian particle nips appropriate hierarchical resulting parameter four paper assignments smc concentration note based processes learning gap independent dpmm without obtain xing step true minutes independently", "authors": "Sinead Williamson; Avinava Dubey; Eric Xing", "thumbnail_path": "thumbnails/Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/broderick13.pdf", "title": "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes", "topics": [0.013227006235854608, 0.013170516400678577, 0.013276515015447795, 0.013197240946941847, 0.013263339496982907, 0.93386538190409429], "most_common_string": "feature data algorithm objective cluster number features clustering algorithms one row clusters means jordan map new learning bayesian random may function four ibp probability kulis problem gibbs model collapsed let gaussian point case prior process znk ghahramani index base values estimate given allocation using posterior consider via pictures run second nonparametric broderick mean matrix shows likelihood also set initializations asymptotics tabletop picture appear counts parameter note stepwise latent penalty local mixture crp faces optimization initialization obtain models choice customer optimal statistical framework since combinatorial nth log assign hyperparameter beta show text equal sampling sampler value assigned pitman form similar even", "authors": "Tamara Broderick; Brian Kulis; Michael Jordan", "thumbnail_path": "thumbnails/MAD-Bayes: MAP-based Asymptotic Derivations from Bayes.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/chuang13.pdf", "title": "Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment", "topics": [0.015948489365931226, 0.015896867874121555, 0.92037295422821563, 0.015881986511232238, 0.016010454391279644, 0.015889247629219689], "most_common_string": "topic topical topics models concepts latent reference model measures figure junk lda using fused domain resolved matching similarity alignment scores concept values david human two matches number correspondence ramage word also dot set optimization rescaled via product hyperparameter blei assessing newman evaluation likelihood pairs probability score intrinsic stanford based information introduce framework one matrix chart diagnostics coherence relevance repeated research large work correlation misalignment modeling mccallum experts within provide parameter steyvers wallach likelihoods corpus considered text manning mimno observe user analysis match range examine results plda entries missing marked trained process axis may measure daniel diagnostic rank quality built random", "authors": "Jason Chuang; Sonal Gupta; Christopher Manning; Jeffrey Heer", "thumbnail_path": "thumbnails/Topic Model Diagnostics: Assessing Domain Relevance via Topical Alignment.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/sutskever13.pdf", "title": "On the importance of initialization and momentum in deep learning", "topics": [0.92772770316078834, 0.014430886490428495, 0.014493900515642423, 0.014490900899170028, 0.014452342955275569, 0.014404265978695145], "most_common_string": "momentum learning nag deep methods neural results training martens gradient convergence networks optimization initialization use rate method problems used sutskever directions hinton random local see table performance using rnns stochastic along may error previous particular parameter found standard achieve update even type information initializations objective importance recurrent convex scale certain sgd thus train large also however many bengio problem tasks accelerated one phase velocity set descent units like experiments nesterov cient quadratic constant better curvature given coe much schedule appendix dynamics errors work values temporal across dnns larger make without seems classical models initialized reported allows hidden spectral proceedings achieved", "authors": "Ilya Sutskever; James Martens; George Dahl; Geoffrey Hinton", "thumbnail_path": "thumbnails/On the importance of initialization and momentum in deep learning.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/georgiev13.pdf", "title": "A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines", "topics": [0.015660133792241029, 0.01565318657415566, 0.015694881483325811, 0.01568861044657854, 0.92164830595827718, 0.015654881745421817], "most_common_string": "model rbm visible ratings models hidden collaborative layer results two prediction rating also hybrid user multinomial items training using better framework units matrix users mae order quality given learning item boltzmann movielens machines proceedings values data one best yields datasets unit figure restricted sarwar salakhutdinov predictions original number filtering rbms algorithms used evaluation average correlations nodes truyen use performance based modeling value real standalone weights layers finally new trained weight shows could see however similar single compared latent opposed work information correlation features table latter joint set binary previous right usa generated wij related shown linear yum connected sum comparable", "authors": "Kostadin Georgiev; Preslav Nakov", "thumbnail_path": "thumbnails/A non-IID Framework for Collaborative Filtering with Restricted Boltzmann Machines.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/wulsin13.pdf", "title": "Parsing epileptic events using a Markov switching process model for correlated time series", "topics": [0.016916284806180917, 0.016866451427527789, 0.016965970900209574, 0.91519574554645522, 0.017081024950900952, 0.016974522368725608], "most_common_string": "state channel event channels model time events series process seizure eeg markov epileptic sample ieeg clinical data transition using dynamics states parsing parameters innovations three fox correlated seizures modeling structure supplement feature switching dynamic bursts set electrode prior one conditional chains sampling number analysis similar graph innovation multivariate via given jordan sequences conference graphical heldout two also sparse models denotes vector onset end nonparametric mcmc observations beta proceedings correlations individual learning covariance hmm bayesian middle capture sequence use statistics posterior spatial consider large work example machine factorial recordings features observation conditioned left dependency shared distributions important independently red independencies predictions", "authors": "Drausin Wulsin; Emily Fox; Brian Litt", "thumbnail_path": "thumbnails/Parsing epileptic events using a Markov switching process model for correlated time series.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/salazar13.pdf", "title": "Exploring the Mind: Integrating Questionnaires and fMRI", "topics": [0.014158890721642807, 0.014118617661040675, 0.014178157079353658, 0.01412388398199607, 0.92924292013562904, 0.014177530420337642], "most_common_string": "model data matrix sparse binary fmri real questions questionnaires text topic amygdala distribution associated graphical prior latent models learned ordered proposed precision consider use figure categorical people features analysis also questionnaire answers via panel section probit integrating joint stimuli using two related exploring topics mind reactivity performance multiple considered network covariance within expressions construction vector based yij salazar factor modeling bayesian results groups predict brain yoshida average factorization responses west used question posterior four types relationships zero ibp parameters visual may response meeds representation shows connectivity given note measured learning employ modeled subjects yields algorithm structure finally updated every component", "authors": "Esther Salazar; Ryan Bogdan; Adam Gorka; Ahmad Hariri; Lawrence Carin", "thumbnail_path": "thumbnails/Exploring the Mind: Integrating Questionnaires and fMRI.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/alain13.pdf", "title": "Gated Autoencoders with Tied Input Weights", "topics": [0.015885150081709699, 0.015778063550706822, 0.016018981335938827, 0.92066715464302207, 0.015860515012394032, 0.015790135376228399], "most_common_string": "factor units network mapping layer images gated learning transformations cga rotations autoencoders matrix transformation angle input rotation memisevic one weights reconstruction error activation since given complex size two values representation pairs case image set see algorithm degree use projections real tied mathematical represent learned product number orthogonal hinton training particular pixels eigenvalues figure better also networks mean cross classical inner work numbers corresponding neural quadrature matrices section algorithms diagonal detection fact eigenvectors would parameters action unit multiplication representations weight spectrum performance proceedings thus approach however fny fnx cosine used less compared like recognition imaginary comparison deep commuting whereas test layers", "authors": "Alain Droniou; Olivier Sigaud", "thumbnail_path": "thumbnails/Gated Autoencoders with Tied Input Weights.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/cho13.pdf", "title": "Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images", "topics": [0.019832444895833884, 0.019776491121726934, 0.020066310010102784, 0.019964293978505087, 0.90060007008544207, 0.019760389908389183], "most_common_string": "image denoising dae simple sparse noise latent hidden sparsity spdae encoder trained proposed sample error images reconstruction decoder representation test autoencoders performance patches given see noisy level improves denoised set using deep corrupted one coding white average case used spdaes model data gaussian learning units without function layers obtained two activation daes patch layer neural samples use small space training number xie target explicitly instance regularizer also applied burger component maps representations explicit clean approach however improvement input larger discriminative type following regularization found especially models code shrinkage large conventional additive cho information max ieee approaches levels nonlinearity pixels autoencoder", "authors": "Kyunghyun Cho", "thumbnail_path": "thumbnails/Simple Sparsification Improves Sparse Denoising Autoencoders in Denoising Highly Corrupted Images.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/gupta13b.pdf", "title": "Natural Image Bases to Represent Neuroimaging Data", "topics": [0.018767566047578763, 0.018735987821109558, 0.9059735515176448, 0.018887180623362681, 0.018871513830938959, 0.018764200159365373], "most_common_string": "mri bases data natural image feature using figure used set neuroimaging mci images brain shows disease represent pooling features performance learning learned visual basis early table representation approach adni patches activations method learn neural dementia parameters however analysis sparse algorithm results matsuda two convolutional sigmoid structural autoencoder imabayashi slice inspection information lesions binary lecun research accuracy sensitivity thus journal validation activation convolution stereotactic diagnostic kloppel clinical scans sae class yang ica median network various extraction layer surface normalization also input three test computerized progression teh autoencoders clinicians section search healthy scan hinton high work values following collection statistical university shown", "authors": "Ashish Gupta; Murat Ayhan; Anthony Maida", "thumbnail_path": "thumbnails/Natural Image Bases to Represent Neuroimaging Data.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/yuhui13.pdf", "title": "Direct Modeling of Complex Invariances for Visual Object Features", "topics": [0.016205816624644119, 0.016187736489693644, 0.91888828275165868, 0.016228450859390956, 0.016279628435352661, 0.016210084839260022], "most_common_string": "complex invariance learning dictionary feature would pooling features invariances object data modeling direct using visual training size image receptive view layer system set coates representation base accuracy results algorithm networks case amount labeled approach one result performance local patch simple table unsupervised deep rotation additional see transforms like output layers patches experiment method network coding knowledge sparse recognition similar activation full prior vik also best strategy neural per spatial large learn example dataset related directly zou class invariant recent competitive improvement works pooled appear highly use two convolutional give good beyond could encoding believe dtk say gains however much lecun", "authors": "Ka Yu Hui", "thumbnail_path": "thumbnails/Direct Modeling of Complex Invariances for Visual Object Features.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/chen13g.pdf", "title": "Spectral Compressed Sensing via Structured Matrix Completion", "topics": [0.91731586459711389, 0.016468377267765327, 0.016709465243826636, 0.016469251924113037, 0.016550227880131028, 0.01648681308704996], "most_common_string": "matrix hankel completion emac enhanced algorithm spectral sensing compressed via data frequency recovery incoherence structured matrices frequencies object model samples theorem form number one condition set signal candes entries theoretical observation problem ieee random conditions norm following based lemma observed sparse processing results small let information singular algorithms rank order value underlying noise denote obtained projection exact reconstruction harmonic suppose numerical possible minimization true time consider phase chi guarantee max imaging onto locations transactions section ground however sampling recht location sparsity super noisy gross models resolution partial stable rate method thresholding applications nonparametric denotes probability size given constant approach experiments", "authors": "Yuxin Chen; Yuejie Chi", "thumbnail_path": "thumbnails/Spectral Compressed Sensing via Structured Matrix Completion.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/papailiopoulos13.pdf", "title": "Sparse PCA through Low-rank Approximations", "topics": [0.013289595228153114, 0.93385620193382468, 0.013286093577503117, 0.013181122288752241, 0.013212277192182991, 0.013174709779584026], "most_common_string": "sparse algorithm pca data approximation principal set support matrix vector obtain component max largest vectors two zhang intersection optimal decay candidate sparsity spannogram case sets eigenvalue entries use one fullpath supports twitter eigenvector method approximations performance words step elimination tpower curves used analysis possible ghaoui unit main absolute fact eigenvectors using papailiopoulos constant elements international end machine experiments pcs exactly show covariance opt running power journal time moghaddam desired table greek ieee tweets matrices given scheme points google equal simply learning observe yuan guarantees output bound log problem test microsoft maximum greeceg solution indices eigenvalues love work compare information feature", "authors": "Dimitris Papailiopoulos; Alexandros Dimakis; Stavros Korokythakis", "thumbnail_path": "thumbnails/Sparse PCA through Low-rank Approximations.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/xiang13.pdf", "title": "Efficient Sparse Group Feature Selection via Nonconvex Optimization", "topics": [0.01656167725472784, 0.016499201068403298, 0.91732837560912872, 0.016536138058670231, 0.016542588013383524, 0.016532019995686414], "most_common_string": "group selection sparse feature nonconvex method projection lasso convex algorithm optimization following methods groups solution optimal parameter via algorithms features results set huang proposed section data number oracle two statistical performance learning zhang max journal estimator admm constraints table however moreover model theorem constraint log function problem formulation parameters end gradient given also objective result approach tuning mcp shen step therefore accelerated breheny solving since bridge boyd based minimize nonzero global whose consistent selected composite hold values machine may minimizer paper programming assumption note bisection subject sglp due value return used less agm time computation high discussion theory accuracy addition", "authors": "Shuo Xiang; Xiaotong Shen; Jieping Ye", "thumbnail_path": "thumbnails/Efficient Sparse Group Feature Selection via Nonconvex Optimization.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/gong13a.pdf", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems", "topics": [0.016604229128637758, 0.01647006892916977, 0.016588609144332581, 0.016547982862735743, 0.91726796552478362, 0.016521144410340492], "most_common_string": "algorithm problem min line search function gist objective criterion arg iterative general value convex log shrinkage following thresholding sparse proposed sequence step convergence cpu time analysis problems zhang sign scaled max data size seconds monotone lemma optimization solve learning solution two proof used initialize iteration thus sets theorem journal via propose many point proximal using wright critical bounded commonly outer solving university assumption obtain operator limit gong rule scp monotonically penalties satisfy let assumptions algorithms paper class based tmin regularizers points solves hold machine applications ieee easily related regularizer experiments however table present also follows signal transactions considering processing computational", "authors": "Pinghua Gong; Changshui Zhang; Zhaosong Lu; Jianhua Huang; Jieping Ye", "thumbnail_path": "thumbnails/A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/chen13h.pdf", "title": "Robust Sparse Regression under Adversarial Corruption", "topics": [0.92472799224413982, 0.015029919304571501, 0.015080677365561885, 0.01504717121931278, 0.015074606414988036, 0.015039633451426041], "most_common_string": "robust regression corruption algorithm corrupted support sparse lasso model recovery matrix number algorithms outliers one log consider even standard dantzig force convex brute entries rotr following show response selector performance many theorem noise inner high outlier linear also error guarantees adversarial setting least approach simple optimization note using rows covariates case results chen use ieee trimmed correct recover output design pursuit particular figure parameter might statistics covariate min distributed set well loss methods caramanis problem problems random second section approaches product natural may data observations transactions let handle fail tsybakov true models assume row errors two example thresholding gaussian parameters", "authors": "Yudong Chen; Constantine Caramanis; Shie Mannor", "thumbnail_path": "thumbnails/Robust Sparse Regression under Adversarial Corruption.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/dimitrakakis13.pdf", "title": "ABC Reinforcement Learning", "topics": [0.018626312807927019, 0.018554680403253952, 0.018713278133991776, 0.018561104060197349, 0.01868709704511811, 0.90685752754951188], "most_common_string": "abc reinforcement learning policy bayesian approximate posterior model statistic environment value number good using prior may use however lspi inference sampling samples sampled simple utility parameters history data class pendulum used computation probability trajectories even optimal ntrj framework policies ndat problem algorithm markov sample better lagoudakis given simulators thompson set dimitrakakis approach rollouts general bertsekas theorem simulator decision additional time via goal parametrised since sequence distribution expected methods simulation function models problems finally case results two car statistical standard estimate need icml also fact well real statistics generated consider vlassis dynamic advantage strens approximation action proof would process particular poupart", "authors": "Christos Dimitrakakis; Nikolaos Tziortziotis", "thumbnail_path": "thumbnails/ABC Reinforcement Learning.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/cuturi13.pdf", "title": "Mean Reversion with a Variance Threshold", "topics": [0.015146866143202924, 0.015050306389611511, 0.015140497222904937, 0.92447497886890306, 0.015092539426199774, 0.015094811949177934], "most_common_string": "variance mean trading problem using reversion basket process baskets costs threshold time vector optimal portmanteau crossing given transaction series box minimize predictability solution figure solving sharpe techniques statistic section criteria matrix cointegration stationary assets order eigenvalue tiao subject show minimizing three two weights program multivariate unit asset one average ratio volatility contract johansen ols problems results def sample matrices yang solutions computed nemirovski convex relaxation strategy mathematical per noise form consider cost cents following jurek variables relaxations estimation thus analysis solve also arbitrage classical bound detailed variable reverting exact resulting programming write produce data paper since autoregressive hence generalized measures", "authors": "Marco Cuturi; Alexandre dAspremont", "thumbnail_path": "thumbnails/Mean Reversion with a Variance Threshold.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/wilson13.pdf", "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation", "topics": [0.016206193806124373, 0.016098661295197013, 0.016266302267674042, 0.016195792374938511, 0.016200665460862889, 0.91903238479520311], "most_common_string": "kernel kernels gaussian data process spectral pattern function figure using training functions covariance used density squared rasmussen exponential patterns learned learning williams shown discovery predictive processes sinc stationary extrapolation mean popular peak mixture long machine correlation simple bayesian inference discover one performance neural components model airline periodic marginal red negative likelihood months passenger structure term extrapolate could points black densities human distribution covariances small trend hyperparameters features given exp however many mixtures rational large expressive section component frequency shows log also region proposed adams noise quadratic true models every positive gaussians standard number gps wilson class closed moreover empirical blue", "authors": "Andrew Wilson; Ryan Adams", "thumbnail_path": "thumbnails/Gaussian Process Kernels for Pattern Discovery and Extrapolation.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/grinberg13.pdf", "title": "Average Reward Optimization Objective In Partially Observable Domains", "topics": [0.016663879588509366, 0.91686319653066872, 0.016669596349790407, 0.016535068331766753, 0.01664232197320812, 0.016625937226056698], "most_common_string": "reward policy state linear average psr function stationary process prp system action let distribution systems representation predictive pomdp states observable behavior represented example one markov control framework probability partially psrs stochastic based domains following hidden optimization actions examples two represent search rational given result theorem vector objective fact learning without dimension using memory since induced number observations rewards planning parameters conference policies setting whose changes might evolution matrix future singh proceedings international sequences corresponding underlying properties consider particular figure mean hence also observation processes ergodic property well analysis model appropriate sequence called section jaeger respect obtained mao complexity james show", "authors": "Yuri Grinberg; Doina Precup", "thumbnail_path": "thumbnails/Average Reward Optimization Objective In Partially Observable Domains.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/vanseijen13.pdf", "title": "Planning by Prioritized Sweeping with Small Backups", "topics": [0.019824739247109504, 0.019762626291337113, 0.9008460524075218, 0.019762963013535521, 0.019833279408803178, 0.019970339631692747], "most_common_string": "backup backups full small state time successor value planning model reversed per update usa one performance nsa cient method action learning using number states based methods sample prioritized optimal policy rsa psa step performs task reinforcement moore algorithm values used computation complexity computational mdps equation sweeping estimate pair reward function estimates second section initialize priority note demonstrate new since average predecessor case single queue memory following figure performed episodes transition version error equal atkeson results evaluation large current domains composite agent peng return introduce instead pairs times hence wiering williams observed cycles require expected decision use goal greedy top rewards", "authors": "Harm van Seijen; Rich Sutton", "thumbnail_path": "thumbnails/Planning by Prioritized Sweeping with Small Backups.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/wu13.pdf", "title": "Dynamic Covariance Models for Multivariate Financial Time Series", "topics": [0.020103920383579381, 0.019974245195230892, 0.020066526637296491, 0.020133175922400166, 0.89968272105362168, 0.020039410807871343], "most_common_string": "bekk bmdc time model models predictive particle multivariate series dynamic covariance method data parameters performance inference likelihood number bayesian methods process parameter experiments mean gwp daily section returns average posterior aud standard used figure financial particles wishart generalized proposed regularized matrices cost table rapf dataset gaussian previous journal ghahramani market garch eur wilson equity case using step finally stochastic estimates prediction engle brl predictions values shows diagonal volatility statistical best however empirical jpy computational distribution auxiliary algorithm results novel large sequential shown training autoregressive times also set importance analyzed comparison datasets maximum shrinkage zero heteroscedastic changes plot given following performed", "authors": "Yue Wu; Jose Miguel Hernandez-Lobato; Ghahramani Zoubin", "thumbnail_path": "thumbnails/Dynamic Covariance Models for Multivariate Financial Time Series.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/hocking13.pdf", "title": "Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression", "topics": [0.016986812314212792, 0.016909383492617138, 0.017009939877105328, 0.016933577767265423, 0.91521110449984977, 0.016949182048949459], "most_common_string": "using model log data annotation penalty learning loss function interval error signal regression number surrogate detection table optimal term problem figure annotated smoothing penalties target margin signals learn set use convex functions features algorithm annotations complexity shown since exp models segments feature max used noise variance sparse one arg plausiblek calculate two selection minimizing several changes sets bic kmax constant every panel visual may regions estimate mbic points line four vector estimated section learned method segmentation size changepoint min show however many uses results exact relaxation training criterion propose found separable note algorithms multiple lavielle expert drawn limits hocking database", "authors": "Toby Hocking; Guillem Rigaill; Jean-Philippe VERT; Francis BACH", "thumbnail_path": "thumbnails/Learning Sparse Penalties for Change-point Detection using Max Margin Interval Regression.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/willemvandemeent13.pdf", "title": "Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data", "topics": [0.013408407281319765, 0.01337769026053812, 0.93290624234569652, 0.013422331981812675, 0.01347161172219727, 0.013413716408435607], "most_common_string": "time states model data inference series bayes set state models number log estimation learning variational empirical posterior veb analysis hyperparameters kinetic lower markov ensemble hidden experimental results rates simulated bound parameters transition single two experiments shows inferred selection hmms prior figure approach terms maximum distribution type known lveb likelihood means expectation individual bayesian procedure used methods graphical method conformational gonzalez molecule smfret consensus form similar use machine transitions shown could histograms hyperparameter show mixture using respect distributions bishop whereas algorithm biophysical maximization obtained parameter energy given observations jordan also shared error parametric bronson yields observables level experiment along fei equation", "authors": "Jan-Willem Van de Meent; Jonathan Bronson; Frank Wood; Ruben Gonzalez, Jr.; Chris Wiggins", "thumbnail_path": "thumbnails/Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/ganeshapillai13.pdf", "title": "Learning Connections in Financial Time Series", "topics": [0.91141488203038035, 0.01764305330679709, 0.017717668438268872, 0.017774207307241316, 0.017747363714861981, 0.017702825202450201], "most_common_string": "returns return model equities portfolio correlation matrix using method daily connectedness time market portfolios learning expected series risk large use two equity extreme used given factor connections weights financial day methods learn sharpe table events journal covariance positive therefore figure sector set cov optimization multivariate values map days historical cumulative negative problem partial learned losses ratio data sectors energy since least also construction fac cost minimum based statistical among precision regression built maximum results list active crisis information may pcr companies evcr measures sensitivity variance build average models relationships solnik work analysis squares estimate investors min performance bac parameters new", "authors": "Gartheeban Ganeshapillai; John Guttag; Andrew Lo", "thumbnail_path": "thumbnails/Learning Connections in Financial Time Series.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/bugraerol13.pdf", "title": "The Extended Parameter Filter", "topics": [0.01552491105589429, 0.01545683549657012, 0.015547005267022599, 0.92242726360498306, 0.015548425660442185, 0.015495558915087825], "most_common_string": "parameter model time approximation transition density particles particle algorithm gibbs mean approximate epf figure order models parameters polynomial extended function taylor section state sampling may distribution process sample values static true kalman filter converges statistics separable variables note observation storvik carlo method following sir one system converge approach arbitrary posterior deviation update monte let use sin statistical matrix space shows case bayesian step general degeneracy standard data series gaussian given estimation learning value theorem using per problem sequential inference known equation dynamical complexity markov however approximations doucet show respect applied sequence supplementary statistic log form constant assume cauchy requires dynamic", "authors": "Yusuf Bugra Erol;", "thumbnail_path": "thumbnails/The Extended Parameter Filter.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/han13a.pdf", "title": "Transition Matrix Estimation in High Dimensional Time Series", "topics": [0.016684770318429309, 0.016488149944581889, 0.016644998724161409, 0.91701050213422142, 0.016596570902480092, 0.016575007976125922], "most_common_string": "matrix transition var method data let equation estimation time lasso models vector high series dimensional autoregressive model methods paper ridge represent section max new norm following log norms proposed estimator matrices covariance sparsity stationary averaged frobenius estimating linear penalty respect three analysis liu using problem results synthetic entries level analyzing sample theoretical provide stock lag propose induced result pattern nonzero provided convergence argmin existing dimensionality multivariate table brain statistical number one minj points subject losses empirical optimization error journal han moreover comparison compared prediction sign marginal work figure process introduce gaussian support size asymptotic doubly maxj performance selection show set", "authors": "Fang Han; Han Liu", "thumbnail_path": "thumbnails/Transition Matrix Estimation in High Dimensional Time Series.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/chen13i.pdf", "title": "Dependent Normalized Random Measures", "topics": [0.92312439581184047, 0.015296543553329919, 0.015386737240540451, 0.01530348700741083, 0.015485241294553002, 0.015403595092325282], "most_common_string": "random normalized qrt process measures dependent sampler time gamma models slice number hmngg marginal topic probability set measure atoms htngg datasets parameter region dirichlet posterior see nrms crm mngg also independent teh tngg crms tnrm distribution two zrtk wrk distributed mnrm documents poisson appendix thinning variables observations construction nrm used samplers weights following stl intensity thus model lin thinned ess work times mixture vtl using spatial test larger sample xtl data nonparametric training rao constructions generalized class ngg words associated bayesian hdp assigned follows icml hierarchical atom hmngp allowing tpami index rate base resulting inference call marginally values regions james", "authors": "Changyou Chen; Vinayak Rao; Yee Whye Teh; Wray Buntine", "thumbnail_path": "thumbnails/Dependent Normalized Random Measures.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/ding13.pdf", "title": "Topic Discovery through Data Dependent and Random Projections", "topics": [0.015900340836926874, 0.01576904925221842, 0.015836566679049473, 0.015768959682250179, 0.92092526970988875, 0.015799813839666318], "most_common_string": "novel topic words algorithm matrix word topics random ddp arora set gibbs algorithms data points projections extreme document convex dataset two number documents dependent nmf distinct clustering complexity error proposition given also approach discovery extracted since probability blei learning steyvers maximum table iid training ground modeling swimmer patterns image zzz provable projection work following example figure sample end correspond shown one separability multiple based direction truth distribution estimated method latent parameters apply positive values tan dirichlet nonnegative show asymptotically present empirical analysis proposed input using output images similar associated single clean synthetic condition diag section body let machine order positions", "authors": "Weicong Ding; Mohammad Hossein Rohban; Prakash Ishwar; Venkatesh Saligrama", "thumbnail_path": "thumbnails/Topic Discovery through Data Dependent and Random Projections.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/gupta13a.pdf", "title": "Factorial Multi-Task Learning : A Bayesian Nonparametric Approach", "topics": [0.014680633818352662, 0.014649873436911463, 0.014792626069630875, 0.014661030976291307, 0.92653460231473572, 0.01468123338407799], "most_common_string": "tasks learning task model process subspace groups data using beta number group hierarchical prior predictors bayesian regression nonparametric bases posterior factor shared set training used dirichlet use datasets machine factorial framework related joint across sampling analysis gupta varying iii given performance relatedness distribution results synthetic dataset however real sharing multiple modeling second proposed index degree allows figure rmse inferred approach basis gibbs school method multitask passos individual conference bernoulli journal two jointly inference applications experiments one argyriou stl see problem automatically research unrelated ztk construction hbp therefore kumar xti share sample feature kang matrix partition mixture written fut zgt examples", "authors": "Sunil Gupta; Dinh Phung; Svetha Venkatesh", "thumbnail_path": "thumbnails/Factorial Multi-Task Learning : A Bayesian Nonparametric Approach.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/reed13.pdf", "title": "Scaling the Indian Buffet Process via Submodular Maximization", "topics": [0.016548952572565587, 0.016466881749960818, 0.016640426184713751, 0.016483914635041397, 0.016611205418866709, 0.91724861943885161], "most_common_string": "submodular ibp inference meibp latent variational models methods dataset model algorithm nonnegative maximization data function indian priors solution via process feature map complexity ghahramani matrix features evidence using distribution time prior ugibbs znk bnmf gaussian show solutions converged lower aibp bound true mfvb sampling results likelihood given optimization binary used factors updates large test akd scaling kan random local rvs synthetic figure optimal number also set however convergence equivalence following vibp kbn obtains shows piano columns flickr bayesian obtain sparse assignments submodularity maximum global welling small let kurihara larger use framework space see optimum terms independent maximizing yields truncated approximate", "authors": "Colorado Reed; Ghahramani Zoubin", "thumbnail_path": "thumbnails/Scaling the Indian Buffet Process via Submodular Maximization.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/kim13.pdf", "title": "A Variational Approximation for Topic Modeling of Hierarchical Corpora", "topics": [0.016135454996363634, 0.016164261875734411, 0.016209512491572625, 0.016127696917455758, 0.91918743730918351, 0.016175636409689966], "most_common_string": "topic variational corpora dirichlet tilda model hierarchical hdps models proportions approximation corpus inference topics documents number results lda document parameters latent one blackhatworld teh section also blei modeling gibbs bound categories variables approach sampling learning figure subcategories wang category log freelancer large prior two nonparametric online proceedings job distribution lower deep nodes work root paper parent conference levels algorithm represent zdn machine applications developed main approaches international see terms however attached many procedure nips describes structure finally postings use information collection inequality related used special internet tokens note words set graphical bayesian web opt articles methods form node draw four", "authors": "Do-kyum Kim; Geoffrey Voelker; Lawrence Saul", "thumbnail_path": "thumbnails/A Variational Approximation for Topic Modeling of Hierarchical Corpora.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/kim13a.pdf", "title": "Manifold Preserving Hierarchical Topic Models for Quantization and Approximation", "topics": [0.019272754150146121, 0.019203403145424045, 0.019506598587559772, 0.90342799808527008, 0.019396032932441222, 0.019193213099158633], "most_common_string": "manifold data samples sampling quantization input topic source figure plsi mixture proposed topics training interpolation sparse number preserving models convex separation using random overcomplete hull model latent parameters set hierarchical method neighbors approximation use two points also sparsity results better one layer cross speech variable original second learned vectors rates entropy ordinary parameter corners linear sum selection reconstruct rate case representation note probabilistic estimation best sources digit provide systems matrix nts estimate sets class neighboring proceedings whole analysis provides additional weights four blue reconstruction inputs performance combination shows first instance get new result terms however smaragdis learning conference three less", "authors": "Minje Kim; Paris Smaragdis", "thumbnail_path": "thumbnails/Manifold Preserving Hierarchical Topic Models for Quantization and Approximation.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/das13.pdf", "title": "Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically ", "topics": [0.017357531361933722, 0.017360365083221273, 0.017306659827534387, 0.017258710802772623, 0.017487535725767223, 0.91322919719877071], "most_common_string": "topic topics subtle software concerns stm models document dirichlet process ftm hdp number discovering detect coherence vectors used using code subtly words gsbp automatically distribution word recall concern proceedings inference may documents manifested dataset two sample standard corpus sentences also generalized however problem conference model source rare discover perplexity use detected detecting across level international jhotdraw sentence one table approach distributions berkeleydb section prior stick breaking bdai note average case thus binary important program small rarely example occur propose probability dos datasets observed ability extent well keywords latent cases end nips engineering denotes top gold text conditional due empirical lda", "authors": "Mrinal Das; Suparna Bhattacharya; Chiranjib Bhattacharyya; Gopinath Kanchi", "thumbnail_path": "thumbnails/Subtle Topic Models and Discovering Subtly Manifested Software Concerns Automatically .jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/weinshall13.pdf", "title": "Latent Dirichlet Allocation Topic Model with Soft Assignment of Descriptors to Words", "topics": [0.017281269038237124, 0.91324628039365785, 0.017506674216084921, 0.017246318603175395, 0.017428996550792969, 0.017290461198051707], "most_common_string": "model words lda assignment dictionary word video soft descriptors topic events log using probability variational generative detection represented parameter one documents mixture described distribution descriptor learning document extended use set inference algorithm given blei estimation vector order original mahadevan section data hard latent novelty parameters representation used size state bag bags fdn similar models novel fdnj following collection hidden training dirichlet features observed online exp dataset dynamic obtained probabilities visual event topics frame respect thus cvpr performance method therefore may likelihood isa corpus continuous sivic vision achieved modeling lower identify fdna image methods function histogram update patches level discrete new", "authors": "Daphna Weinshall; Gal Levi; Dmitri Hanukaev", "thumbnail_path": "thumbnails/Latent Dirichlet Allocation Topic Model with Soft Assignment of Descriptors to Words.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/bi13.pdf", "title": "Efficient Multi-label Classification with Many Labels", "topics": [0.017693928056395053, 0.017616003888681848, 0.91168911495870086, 0.017651278719066708, 0.017712696584096712, 0.01763697779305894], "most_common_string": "label number labels data sampling matrix proposed columns error log algorithm boutsidis learning time plst section encoding cssp set many probability table cplst large approximation moplms rank training selected also using methods dmoz machine selection algorithms proceedings thus subset rmse lin best much drineas output results use sample shows however conference trials proposition full sets kernel binary used analysis desc column delicious problem svd prediction international method may space performance various optimization perform obtain transformation takes problems vector obtained ndk computationally approach moreover transformed lebanon step compute balasubramanian zhang hyc even vectors select following tai multilabel randomized given testing rrqr", "authors": "Wei Bi; James Kwok", "thumbnail_path": "thumbnails/Efficient Multi-label Classification with Many Labels.jpg"}, {"pdf_url": "http://jmlr.csail.mit.edu/proceedings/papers/v28/afkanpour13.pdf", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning", "topics": [0.015960004372711663, 0.015903928511420942, 0.9202537647844341, 0.015928814951100698, 0.015979324786423051, 0.015974162593909713], "most_common_string": "algorithm kernel learning kernels method gradient descent large let randomized note mirror set methods problem number mkl convex multiple case kloft algorithms section sampling polynomial also scale machine space see time estimate example complexity base one based however input weights stochastic product notation coordinate computational function use iteration pages cortes results degree compare sample standard performance distribution optimization thus journal underlying bach convergence assume consider research cost training experiments given importance conference empirical predictor denote vector uniform holds data shown shows proceedings approach paper terms value depends using loss works nesterov form volume fact solution penalized linearly particular learn following", "authors": "Arash Afkanpour; Andras Gyorgy; Csaba Szepesvari; Michael Bowling", "thumbnail_path": "thumbnails/A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning.jpg"}, {"pdf_url": "http://jmlr.org/proceedings/papers/v28/zhang13a.pdf", "title": "MILEAGE: Multiple Instance LEArning with Global Embedding", "topics": [0.015241339453243978, 0.015222797951652916, 0.92377420756670969, 0.015230776873950819, 0.015286637574298946, 0.015244240580143613], "most_common_string": "global method local instance positive learning proposed example methods representation bundle instances multiple problem feature bag mileage bij two mil experiments better used one objective function representations dataset misvm traditional embedding optimization solve theorem convex max based negative bags andrews set remp cutting however log results ratio fuduli considered also image datasets svm solving training features given iteration min maxj accuracy text parts whether large insider information complexity shown threat rademacher please margin proximity research joachims vector approximation labeled optimal parameter derived framework conducted employed paper detection otherwise unlabeled lower icml planes novel solution section parameters ratios following proof machine", "authors": "Dan Zhang; Jingrui He; Luo Si; Richard Lawrence", "thumbnail_path": "thumbnails/MILEAGE: Multiple Instance LEArning with Global Embedding.jpg"}]}